{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2de80b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 21:24:48.386549: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d674c034",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR:0@43.088] global persistence.cpp:519 open Can't open file: 'haarcascade_frontalface_default.xml' in read mode\n"
     ]
    }
   ],
   "source": [
    "model = load_model('mask_modle.h5')\n",
    "\n",
    "face_clsfr=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "labels_dict={0:'mask_weared_incorrect', 1:'without_mask',2:'with_mask'}\n",
    "color_dict={0:(0,0,255),1:(255,0,0),1:(0,255,0)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c96ff14",
   "metadata": {},
   "source": [
    "cv2.CascadeClassifier: This is a class in the OpenCV library used for object detection using Haar-like features. It's specifically designed to detect various objects based on pre-trained XML classifiers.\n",
    "\n",
    "('haarcascade_frontalface_default.xml'): This is the file path or the name of the XML file containing the pre-trained model for detecting frontal faces. This XML file includes the necessary information to detect faces using the Haar Cascade method.\n",
    "\n",
    "face_clsfr: This is a variable used to store the initialized face detection classifier.\n",
    "\n",
    "So, this line of code essentially loads a pre-trained model (in this case, a frontal face detection model) into the face_clsfr variable, allowing you to use it to detect faces in images or video frames. Once initialized, you can use this classifier to detect faces by passing images or frames through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b31b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "img=cv2.imread('home-smiling-family.jpg')\n",
    "\n",
    "gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "faces=face_clsfr.detectMultiScale(gray,1.1,5)  \n",
    "\n",
    "for (x,y,w,h) in faces:\n",
    "    \n",
    "    face_img=gray[y:y+w,x:x+h]\n",
    "    resized=cv2.resize(face_img,(100,100))\n",
    "    normalized=resized/255.0\n",
    "    reshaped=np.reshape(normalized,(1,100,100,1))\n",
    "    result=model.predict(reshaped)\n",
    "\n",
    "    label=np.argmax(result,axis=1)[0]\n",
    "      \n",
    "    cv2.rectangle(img, (x,y),(x+w, y+h),(0,0,255),5)\n",
    "   \n",
    "    cv2.putText(img, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,255,255),2)\n",
    "        \n",
    "        \n",
    "cv2.imshow('FAce detection',img)\n",
    "key=cv2.waitKey(0)   \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6c84c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1421f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f1726e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5504d093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0c3bb85",
   "metadata": {},
   "source": [
    "\n",
    "#faces=face_clsfr.detectMultiScale(gray,1.1,7)  \n",
    "\n",
    "\n",
    "face_clsfr: This variable holds the pre-trained face detection classifier, which was initialized using the Haar Cascade Classifier.\n",
    "\n",
    "detectMultiScale: This is a method in OpenCV used for detecting objects of different sizes in the input image. In this case, it's used to detect multiple faces within the image.\n",
    "\n",
    "gray: This is the input image in grayscale format. Usually, face detection algorithms work better on grayscale images. So, before using the detectMultiScale method, the input image (gray) should be converted to grayscale.\n",
    "\n",
    "1.1: This parameter specifies the scale factor. It denotes how much the image size is reduced at each image scale. In this case, a scale factor of 1.1 means the algorithm is looking for faces slightly smaller than the previously detected ones.\n",
    "\n",
    "\n",
    "Scale Factor:\n",
    "It is a parameter that specifies how much the image size is reduced at each image scale.\n",
    "When the algorithm is looking for objects in the image, it doesnâ€™t know the size of the objects in advance. The scaleFactor helps to compensate for this by resizing the image multiple times to detect objects of different sizes.\n",
    "For example, a scaleFactor of 1.1 means that the algorithm will gradually reduce the size of the image by 10% (1.1 times smaller) at each scale.\n",
    "A smaller scaleFactor results in a slower detection process but potentially more accurate results for smaller objects or faces that are far away. Conversely, a larger scaleFactor speeds up the detection process but might miss smaller or distant objects.\n",
    "\n",
    "\n",
    "\n",
    "7: This parameter represents the minimum number of neighboring rectangles required for a detected face to be retained. Higher values will yield fewer detections but with higher confidence.\n",
    "\n",
    "faces: This variable stores the detected faces' coordinates within the image. It will typically contain information about the rectangles where faces are detected. Each rectangle is represented by its coordinates (x, y) for the top-left corner and the rectangle's width and height (w, h).\n",
    "\n",
    "Overall, this line of code uses the detectMultiScale method with the pre-trained face detection classifier (face_clsfr) to identify faces within the grayscale image (gray) and stores information about the detected faces in the faces variable for further processing or visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6cc249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c520bec",
   "metadata": {},
   "source": [
    "face_img=gray[y:y+w,x:x+h]\n",
    "\n",
    "\n",
    "gray: This is the input image in grayscale format.\n",
    "\n",
    "y:y+w and x:x+h: These slicing operations are used to define the region of interest (ROI) within the image. They extract the portion of the image where the face was detected, based on the coordinates obtained from the detectMultiScale method.\n",
    "\n",
    "y and x represent the starting y-coordinate and x-coordinate of the detected face's top-left corner, respectively.\n",
    "\n",
    "w and h represent the width and height of the detected face's bounding box, respectively.\n",
    "\n",
    "So, gray[y:y+w, x:x+h] effectively selects the portion of the grayscale image where the detected face is located.\n",
    "\n",
    "face_img: This variable stores the extracted face region from the grayscale image. After this line of code is executed, face_img will contain only the pixels corresponding to the detected face within the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a7337a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ad9a972",
   "metadata": {},
   "source": [
    "cv2.rectangle(img, (x,y),(x+w, y+h),(0,0,255),5)\n",
    "\n",
    "\n",
    "cv2.rectangle: This is a function in OpenCV used to draw rectangles on images.\n",
    "\n",
    "img: This is the image on which the rectangle will be drawn.\n",
    "\n",
    "(x, y) and (x+w, y+h): These are the coordinates used to define the rectangle.\n",
    "\n",
    "(x, y) represents the top-left corner of the rectangle.\n",
    "(x+w, y+h) represents the bottom-right corner of the rectangle.\n",
    "x, y, w, and h were obtained from the face detection process and represent the coordinates and dimensions of the detected face region.\n",
    "(0, 0, 255): This tuple (B, G, R) represents the color of the rectangle. Here, it's (0, 0, 255), which means it's a red rectangle. The values in OpenCV's rectangle function are in BGR (Blue, Green, Red) format.\n",
    "\n",
    "5: This parameter specifies the thickness of the rectangle's border. In this case, the thickness is set to 5 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee52849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33b10163",
   "metadata": {},
   "source": [
    "cv2.putText(img, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(0,255,255),2)\n",
    "\n",
    "\n",
    "cv2.putText: This function in OpenCV is used to write text on an image.\n",
    "\n",
    "img: This is the image on which the text will be written.\n",
    "\n",
    "labels_dict[label]: This part fetches the text to be written on the image. The labels_dict is likely a dictionary containing labels or names corresponding to the detected face. label is the identifier or index associated with a particular face detected in the image. This line retrieves the label or name for the detected face from the dictionary.\n",
    "\n",
    "(x, y-10): These are the coordinates where the text will be placed. (x, y-10) positions the text slightly above the detected face. x and y are the coordinates of the top-left corner of the detected face bounding box.\n",
    "\n",
    "cv2.FONT_HERSHEY_SIMPLEX: This is the font type used for the text. FONT_HERSHEY_SIMPLEX is one of the available font styles provided by OpenCV.\n",
    "\n",
    "0.8: This parameter specifies the font scale factor. It determines the size of the text. Here, the text will be 80% of the size of the normal text.\n",
    "\n",
    "(0, 255, 255): This tuple (B, G, R) represents the color of the text. Here, it's (0, 255, 255), which means it's yellow (as OpenCV uses the BGR color format).\n",
    "\n",
    "2: This parameter specifies the thickness of the text. In this case, the text will have a thickness of 2 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2358d07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e10b92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc8e18a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f70a102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710a2a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
