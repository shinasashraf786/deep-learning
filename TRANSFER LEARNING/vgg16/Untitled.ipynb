{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb623357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-29 13:37:39.001149: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.layers import Input, Dense,Flatten\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c38f5e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [224,224]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "984528c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainpath = 'dataset//maskdataset'\n",
    "test_path = 'dataset//testmaskdataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d313c1d",
   "metadata": {},
   "source": [
    "weights='imagined'This parameter specifies the weights to be used in the network.'imagined'refers to the weights pre-trained on the ImageNet dataset\n",
    "Pretrained weights are learned representations from a vast dataset that can help the model generalize better on new,unseen data\n",
    "\n",
    "include_top=False:The parameter specifies whether to include the fully connected layers at the top of the network.When set to False.It means that the fully\n",
    "connected layers responsible for classifying ImageNet classes will not be included.This is often used when you want to add your own custom layers for a different\n",
    "classification task ot when using the network for feature classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21600d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG16(input_shape=IMAGE_SIZE +[3],weights='imagenet',include_top = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0db6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dont train existing weights\n",
    "for layer in vgg.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1027de3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#our layers - you can add more if you want\n",
    "x = Flatten()(vgg.output)\n",
    "\n",
    "x = Dense(150,activation='softmax')(x)\n",
    "\n",
    "x = Dense(100,activation='softmax')(x)\n",
    "\n",
    "prediction = Dense(3,activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfe214fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs = vgg.input,outputs = prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dd7b863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 150)               3763350   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               15100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 303       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18493441 (70.55 MB)\n",
      "Trainable params: 3778753 (14.41 MB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "\n",
    "Model: \"model_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83338d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed715acc",
   "metadata": {},
   "source": [
    "the batch size refers to the number of training examples utilized In one Iteration (one forward and one backward pass) during the training process,\n",
    "nachine learning and deep learning, the batch size refers to the number of training examples utilized in one iteration (one forward and one backward pass) during the training process.\n",
    "training set - train _datagen. flow_from directory(trainpath, target_size-(224,224), batch_size=20)\n",
    "The batch_size-20 parameter signifies that during each iteration of the training process, the model will process and learn fron 20 images fron the training dataset.\n",
    "Training with a larger batch size can potentially increase computational efficiency because it processes multiple samples simultaneously, leveraging parallelisa in modern hardware like GPUS.\n",
    "It may exhibit more stable convergence due to the averaging of gradients across more samples.\n",
    "Smaller Batch Size:\n",
    "Smaller batch sizes may be preferred in scenarios where menory constraints exist, allowing models to be trained on hardware with limited memory.\n",
    "It might provide nore noisy gradients due to the limited sample size per iteration but can sometimes lead to better generalization by exploring a larger variety of training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "615d7c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8982 images belonging to 3 classes.\n",
      "Found 6001 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,shear_range = 0.2,zoom_range = 0.2, horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(trainpath,target_size = (224,224),batch_size =20)\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(test_path,target_size = (224,224),batch_size =20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b108c5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "449.1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8982/20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe394d0c",
   "metadata": {},
   "source": [
    "Keras fit and Keras fit generator functions used to train a deep learning neural network\n",
    "\n",
    "fit is used when the entire training dataset can fit into memory and no data augmentation is applied.\n",
    "\n",
    "fit generator is used when either we have a huge dataset to fit into our memory or when data augmentation needs to \n",
    "\n",
    "be applied.\n",
    "\n",
    "fit generator:This is a function used in some deep learning libraries,like Keras,to train a model using data \n",
    "\n",
    "generated batch-wise by a Python generator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc5b623",
   "metadata": {},
   "source": [
    "fit method:\n",
    "\n",
    "This method is used when the entire dataset can it into memory. I accepts NumPy arrays as Input. You provide the complete training data (train, y_train) directly to the fit method. It's convenient for smaller datasets that can be loaded entirely into memory.\n",
    "model.it(x_train. y_Jrain, epochs=5, batch_size=32, validation_data=(X_vai, Y_val))\n",
    "\n",
    "81 generator method:\n",
    "This method is used when dealing with farger datasets that dont fil into memory. It works with Python generators that yield batches of data. You create Python generator functions to load and preprocess data in batches, which are then fed to the model during training. It's useful for handing large datasets, data augmentation, or when loading data from extemal sources during training.\n",
    "modeL.# generator(training generator, epochs=5, steps_per_ epoch-len(train_ samples)/batch_size, vaikdation_data-validation_ generator, validation_ steps-len(validation_samples y/batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fe1ebc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1796.4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1796.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0cbb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4g/m51tcqcx2jv621yd7trnjv5w0000gn/T/ipykernel_6859/3111667481.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  r = model.fit_generator(training_set, validation_data=test_set,epochs=5,steps_per_epoch= len(training_set),validation_steps = len(test_set))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "450/450 [==============================] - 4554s 10s/step - loss: 1.0598 - accuracy: 0.5179 - val_loss: 0.9819 - val_accuracy: 0.6417\n",
      "Epoch 2/5\n",
      "450/450 [==============================] - ETA: 0s - loss: 0.8592 - accuracy: 0.6303  "
     ]
    }
   ],
   "source": [
    "r = model.fit_generator(training_set, validation_data=test_set,epochs=5,steps_per_epoch= len(training_set),validation_steps = len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61e0e2f",
   "metadata": {},
   "source": [
    "steps_per_epoch = len(training_set): This parameter determines the number of steps (batches) to be considered as one epoch. It's typically set to the total\n",
    "number of samples in the training dataset divided by the batch size. This ensures that the model goes through the entire training dataset once within each epoch.\n",
    "validation_steps = lenfies set): Similar to steps_per _epoch, this parameter defines the number of steps (batches) to yield from the validation generator in one\n",
    "epoch. It's set to the total number of validation samples divided by the batch size.\n",
    "1= model.It generator.): The output of the fil generator function is typically a history object (r in this case), which contains information about the training process, such as training loss, validation loss, aocuracy, etc., for each epoch. This object can be used to visualize or analyze the training process after the\n",
    "model has finished training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0847906a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss\n",
    "\n",
    "plt.plot(r.history['loss'],label='train loss')\n",
    "plt.plot(r.history['val_loss'],label='val loss')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe2d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracies\n",
    "\n",
    "plt.plot(r.history['accuracy'],labels='train acc')\n",
    "plt.plot(r.history['val_accuracy'],label='val acc')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168cfd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from keras.model import load_model\n",
    "\n",
    "model.save('facefeatures_new_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30517f81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
