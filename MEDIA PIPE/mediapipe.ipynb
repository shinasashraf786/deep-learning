{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8838b59",
   "metadata": {},
   "source": [
    "MediaPipe is an open-source framework developed by Google that offers customizable machine learning (ML) pipelines to process media data such as images, video, and audio. It provides a wide range of pre-trained ML models and components to perform various tasks, including:\n",
    "\n",
    "1. Pose Estimation: Detecting and tracking human body poses in images and video.\n",
    "2. Hand Tracking: Identifying and tracking hands in images and video.\n",
    "3. Object Detection: Detecting and tracking objects in images and video streams.\n",
    "4. Face Detection: Detecting and tracking faces in images and video.\n",
    "5. Face Mesh: Estimating facial landmarks in real-time.\n",
    "6. Holistic: Combining multiple components like face detection, pose estimation, and hand tracking to create holistic pipelines.\n",
    "7. Selfie Segmentation: Segmenting a person's image from the background in real-time.\n",
    "8. Hair Segmentation: Segmenting hair from images and video.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fc0afa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in /opt/anaconda3/lib/python3.11/site-packages (0.10.14)\n",
      "Requirement already satisfied: absl-py in /opt/anaconda3/lib/python3.11/site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in /opt/anaconda3/lib/python3.11/site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: jaxlib in /opt/anaconda3/lib/python3.11/site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.11/site-packages (from mediapipe) (3.8.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in /Users/apple/.local/lib/python3.11/site-packages (from mediapipe) (4.9.0.80)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in /opt/anaconda3/lib/python3.11/site-packages (from mediapipe) (4.25.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /opt/anaconda3/lib/python3.11/site-packages (from mediapipe) (0.4.7)\n",
      "Requirement already satisfied: CFFI>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jax->mediapipe) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum in /opt/anaconda3/lib/python3.11/site-packages (from jax->mediapipe) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in /opt/anaconda3/lib/python3.11/site-packages (from jax->mediapipe) (1.11.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.11/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mediapipe --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e684b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-27 10:23:05.240817: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3529a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_facedetection=mp.solutions.face_detection \n",
    "mp_drawings=mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bdc9dc",
   "metadata": {},
   "source": [
    "mp_facedetection=mp.solutions.face_detection: This line imports the face detection module from the Mediapipe library and assigns it to the variable mp_facedetection. This module provides functionality for detecting faces in images or video streams\n",
    "mp_drawings=mp.solutions.drawing_utils: This line imports the drawing utilities module from the Mediapipe library and assigns it to the varlable mp_drawings.\n",
    "This module contains functions for drawing annotations, such as bounding boxes or landmarks, on images or video frames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "758c0da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1724734421.678471   56678 gl_context.cc:357] GL version: 2.1 (2.1 INTEL-20.7.2), renderer: Intel(R) Iris(TM) Plus Graphics 640\n"
     ]
    }
   ],
   "source": [
    "face_detection=mp_facedetection.FaceDetection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a004799",
   "metadata": {},
   "source": [
    "The FaceDetection class in the Mediapipe library utilizes a pre-trained machine learning model to detect faces in images or video frames. This model has been trained on a large dataset of annotated images, allowing it to leam features and patterns assoclated with human faces.\n",
    "creating an instance of the face detection model from the Mediapipe library. The line face_detection=mp_facedetection.FaceDetection) initializes an instance of the FaceDetection class,\n",
    "This instance of the FaceDetection class will allow to perform face detection on images or video streams using the functionality provided by the Mediapipe llbrary, then use this face_detection object to detect faces in images or video frames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bfb6cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1724734421.706340   91683 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "/opt/anaconda3/lib/python3.11/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "video=cv2.VideoCapture(0)\n",
    "while True:\n",
    "    suc,img=video.read()\n",
    "    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    result=face_detection.process(img)\n",
    "    img=cv2.cvtColor(img,cv2.COLOR_RGB2BGR)\n",
    "    if result.detections:\n",
    "        for det in result.detections:\n",
    "            \n",
    "            mp_drawings.draw_detection(img,det)\n",
    "    cv2.imshow('FACE',img)\n",
    "    if cv2.waitKey(1) & 0XFF==ord('q'):\n",
    "        break\n",
    "        \n",
    "video.release()\n",
    "cv2.destroyAllWindows()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af658048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[label_id: 0\n",
       " score: 0.8885659575462341\n",
       " location_data {\n",
       "   format: RELATIVE_BOUNDING_BOX\n",
       "   relative_bounding_box {\n",
       "     xmin: 0.39209404587745667\n",
       "     ymin: 0.5205647349357605\n",
       "     width: 0.2354254424571991\n",
       "     height: 0.41853463649749756\n",
       "   }\n",
       "   relative_keypoints {\n",
       "     x: 0.47817090153694153\n",
       "     y: 0.6443884968757629\n",
       "   }\n",
       "   relative_keypoints {\n",
       "     x: 0.5718467235565186\n",
       "     y: 0.6577954888343811\n",
       "   }\n",
       "   relative_keypoints {\n",
       "     x: 0.532436728477478\n",
       "     y: 0.7609246373176575\n",
       "   }\n",
       "   relative_keypoints {\n",
       "     x: 0.5238144397735596\n",
       "     y: 0.8360314965248108\n",
       "   }\n",
       "   relative_keypoints {\n",
       "     x: 0.40303340554237366\n",
       "     y: 0.6522131562232971\n",
       "   }\n",
       "   relative_keypoints {\n",
       "     x: 0.6068379878997803\n",
       "     y: 0.6754410862922668\n",
       "   }\n",
       " }]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d97fdd",
   "metadata": {},
   "source": [
    "face_detection process this code would typically detect faces in the image\n",
    "\n",
    "and store the result in the variable result\n",
    "\n",
    "Reading a Frame: suc, img = video.read() captures a frame from the video feed.\n",
    "\n",
    "Color Conversion (BGR to RGB): img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) converts the image from BGR (OpenCV's default) to RGB (required by MediaPipe).\n",
    "\n",
    "Face Detection: result = face_detection.process(img) processes the image to detect faces.\n",
    "\n",
    "Color Conversion (RGB to BGR): img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) converts the image back to BGR for display with OpenCV.\n",
    "\n",
    "Drawing Face Detections:\n",
    "If faces are detected (if result.detections:), a loop iterates through each detected face.\n",
    "\n",
    "mp_drawings.draw_detection(img, det) draws the detected face on the image.\n",
    "\n",
    "Displaying the Image: cv2.imshow('FACE', img) displays the image with the detected faces.\n",
    "\n",
    "Exit Condition: if cv2.waitKey(1) & 0xFF == ord('q'): breaks the loop and exits if the 'q' key is pressed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620a3ed8",
   "metadata": {},
   "source": [
    "cv2.waitKey(1): Waits for 1 millisecond for a key event.\n",
    "cv2.waitKey(1) & 0xFF: The q operation ensures the result is within the ASCII range.\n",
    "== ord('q'): Compares the result to the ASCII value of 'q' to check if 'q' was pressed.\n",
    "If 'q' is pressed, the loop breaks,ending the video capture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
